% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate-functions.R
\name{estimateModel}
\alias{estimateModel}
\title{Estimate model from single reliable dataset.}
\usage{
estimateModel(model, y, exposure = NULL, weights = NULL,
  filename = NULL, nBurnin = 1000, nSim = 1000, nChain = 4,
  nThin = 1, parallel = TRUE, outfile = NULL, nUpdateMax = 50,
  verbose = TRUE, useC = TRUE)
}
\arguments{
\item{model}{An object of class \code{\linkS4class{SpecModel}},
specifying the model to be fit.}

\item{y}{A \code{\link[dembase:DemographicArray-class]{demographic array}}
holding the outcome data.}

\item{exposure}{A \code{\link[dembase:DemographicArray-class]{Counts}}
object specifying exposure or sample size.}

\item{weights}{A \code{\link[dembase:DemographicArray-class]{Counts}}
object containing weights.}

\item{filename}{The name of a file where output is collected.}

\item{nBurnin}{Number of iteration discarded before recording begins.}

\item{nSim}{Number of iterations carried out during recording.}

\item{nChain}{Number of parallel chains used in simulations.}

\item{nThin}{Thinning interval.}

\item{parallel}{Logical.  If \code{TRUE} (the default), parallel processing
is used.}

\item{outfile}{Where to direct the ‘stdout’ and ‘stderr’ connection
output from the workers when parallel processing.  Passed to function
\code{[parallel]{makeCluster}}.}

\item{nUpdateMax}{Maximum number of iterations completed before releasing
memory.  If running out of memory, setting a lower value than the default
may help.}

\item{verbose}{Logical.  If \code{TRUE} (the default) a message is
printed at the end of the calculations.}

\item{useC}{Logical.  If \code{TRUE} (the default), the calculations
are done in C.  Setting \code{useC} to \code{FALSE} may be useful
for debugging.}
}
\description{
Estimate rates, counts, probabilities, or means for a single
\code{\link[dembase:DemographicArray-class]{demographic array}}.  The
demographic array is treated as observed without error.
}
\section{Model, y, and exposure}{


The model for the contents of the array is specified using function
\code{\link{Model}}.

If \code{model} specifies a Poisson, binomial, or multinomial model,
then \code{y} must have class
\code{\link[dembase:DemographicArray-class]{Counts}}.  If \code{model}
specifies a normal distribution, then \code{y} can have class
\code{\link[dembase:DemographicArray-class]{Counts}} or
\code{\link[dembase:DemographicArray-class]{Counts}}.

\code{y} may include \code{NA}s.  Missing values can be imputed via function
function \code{\link{fetch}}.  If \code{model} specifies a Poisson
distribution, then \code{y} can also have known
\code{\link[dembase:attachSubtotals]{subtotals}}, which
can help with the imputation of the missing values.

An \code{exposure} term is optional in Poisson models, and required
in binomial models.  (For convenience, \code{demest} treats the sample size
parameter in binomial models as kind of exposure.)  A \code{weights} term
is optional in normal models.
}

\section{Output}{


The output from \code{estimateModel} would often be too large to fit into
memory.  \code{estimateModel} therefore departs from the standard R
behavior in the way it handles output.  Rather than returning an object
containing the output, \code{estimateModel} creates a file on disk,
somewhat like a database.

The name and location of the output file is specified using the
\code{filename} argument. The file is just a text file, and can be copied
and moved.

Users extract items from the file using function such as \code{\link{fetch}},
\code{\link{fetchSummary}}, \code{\link{fetchMCMC}}, and \code{\link{fetchFiniteSD}}.

Functions \code{\link{estimateCounts}}, \code{\link{estimateAccount}},
and \code{\link{predictModel}} follow the same strategy for returning
output.
}

\section{nBurnin, nSim, nChain, nThin}{


\code{estimateModel}, \code{\link{estimateCounts}}, and
\code{\link{estimateAccount}} use Markov chain Monte Carlo (MCMC)
methods for inference.  MCMC methods have two stages: burnin and
production.  During the burnin phase, the model moves from an
initial guess at the location of the posterior distribution
towards the true location.  During the production phase, if all goes
well, the model samples from the true posterior distribution.

Parameter \code{nBurnin} specifies the number of iterations that the model
spendss moving away from its initial location. Parameter \code{nSim}
specifies the number of iterations that the model spends sampling from
the posterior distribution.

Collecting every iteration during the production phase would lead to
huge output files.  Instead, the model collects only one in every
\code{nThin} iterations.  The resulting loss in information is relatively
small, since successive iterations are typically highly correlated.

The calculations are run \code{nChain} times, with each chain yielding
a different sample.  As described in the documentation for
\code{\link{fetchMCMC}}, comparing the samples is a way of checking whether the
model has found the posterior distribution. When each chain seems to be
sampling from the same distribution, the model is said to have
converged.

At the end of the estimation process, the \code{estimateModel} and similar
functions pool the results from all the chains to form a single sample.
This sample has \code{floor(nChain * nSim / nThin)} iterations.
}

\section{Parallel}{


Because each chain is run independently, the calculations can be carried out
in parallel.  By default, \code{estimateModel}, \code{\link{estimateCounts}},
and \code{\link{estimateAccount}} run the calculations on separate cores,
using functions from the \pkg{parallel} package. For portability, however,
the \code{parallel} argument is set to \code{FALSE} in the examples in the
online help.
}

\examples{
library(datasets)
admissions <- Counts(UCBAdmissions)
admitted <- subarray(admissions, Admit == "Admitted")
filename <- tempfile()
estimateModel(Model(y ~ Binomial(mean ~ Gender + Dept)),
              y = admitted,
              exposure = admissions,
              file = filename,
              nBurnin = 50,
              nSim = 50,
              nChain = 2,
              nThin = 2)
fetchSummary(filename)
}
\references{
Gelman, A., Carlin, J. B., Stern, H. S.,
Dunson, D. B., Vehtari, A., Rubin, D. B. (2013)
\emph{Bayesian Data Analysis. Third Edition}. Boca Raton: Chapman &
Hall/CRC.
}
\seealso{
\code{\link{estimateCounts}} is similar to \code{estimateModel},
except that \code{y} is not observed directly, but must be inferred from
multiple noisy datasets. \code{\link{estimateAccount}} infers a demographic
account from multiple noisy datasets.  Calculations can be extended using
\code{\link{continueEstimation}}.  Forecasts based on the results
from \code{estimateModel} can be constructed using function
\code{\link{predictModel}}.
}
